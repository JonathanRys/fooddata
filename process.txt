Remove all non-word chars
Tokenize the list - word boundries(define)
lower case the text - unless there are cases
Run a stemmer - remove conjunctions, etc. (Porter stemmer)
Detect similar word via:
  investigate "Levenshtein distance" to detect number of edits between words
  Subset analysis  
Digrams

Use Naive Bayes on the result



I have created "refined lists" of data manually
Take raw data and categorize a given list of ingredients based on this refined lists:

General Processing steps:
  Preprocess ingredients
    Itemize the list - split on commas, semicolons, and/or, and slashes
    translate non-english characters from target languages to english equivelents
    remove all other non-alpha characters
    ??? Might not need to do the rest of this ???
    trim whitespaces
    convert to lowercase
    remove duplicates

  Process the stripped list
    tokenize words
    stem words
    remove stop words
    POS process words
    remove useless tags and POS
  

To process the "refined lists" into "derived results":
  Follow General processing steps (above)
  Manually remove irrelevent or disuasive terms to make the lists better

To process ingredients for a particular product:
  Follow General processing steps (above)
  
  Compare results to previously "derived results"

